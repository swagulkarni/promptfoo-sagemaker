{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Graded Quality Assurance for GlobalMart's Customer Service\n",
    "\n",
    "At GlobalMart, we've mastered code-based testing of our AI customer service - checking if responses contain order numbers, tracking codes, and refund policies. These basic tests are fast and cheap, but they can't evaluate what really matters: the quality of customer interactions.\n",
    "\n",
    "Think about what makes great customer service:\n",
    "- Professional yet friendly tone\n",
    "- Appropriate escalation of sensitive issues\n",
    "- Clear explanations without technical jargon\n",
    "- Empathetic handling of customer frustrations\n",
    "\n",
    "These qualities can't be measured with simple code checks. That's where model-graded evaluations come in.\n",
    "\n",
    "By using one AI model to evaluate another, we can assess these nuanced aspects of customer service. The evaluating model acts like a quality assurance specialist, checking responses against GlobalMart's service standards.\n",
    "\n",
    "## Learning Objectives\n",
    "- Configure model-graded evaluations in Promptfoo for customer service quality\n",
    "- Use LLMs to evaluate response quality and tone against multiple service standards\n",
    "- Optimize customer service prompts using evaluation insights based on quality metrics\n",
    "- Apply multi-criteria grading for complex assessments\n",
    "- Implement automated QA for response tone and content\n",
    "\n",
    "## The Challenge\n",
    "GlobalMart needs to ensure our AI customer service maintains high standards across:\n",
    "- Professional, helpful tone\n",
    "- Clear response boundaries \n",
    "- Appropriate escalation paths\n",
    "\n",
    "## How LLM based evaluation works\n",
    "\n",
    "The process works by giving the evaluator model:\n",
    "- The customer's original question\n",
    "- Our AI's response\n",
    "- GlobalMart's service quality guidelines\n",
    "- Specific evaluation criteria\n",
    "\n",
    "This approach lets us measure what really matters - not just what was said, but how it was said. Let's see how to implement this at GlobalMart!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our prompts in prompts.py\n",
    "As we've done in previous labs, lets create a new file for our prompts called **`prompts.py`** and add the following prompt function to the file.\n",
    "- **`Basic Prompt`:** Simple instruction set that establishes basic role and scope. Minimal guidance on tone or boundaries. Good baseline for testing but lacks specifics on handling edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing prompts.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prompts.py\n",
    "def basic_service_prompt(message):\n",
    "    return f\"\"\"As GlobalMart's customer service AI, provide clear, professional responses.\n",
    "    Help with orders, products, and account issues.\n",
    "    Customer message: {message}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Promptfoo Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Model-Graded Evaluation\n",
    "The llm-rubric assertion in Promptfoo allows you to use an LLM to evaluate the output of your main model based on specified criteria. This is particularly useful for assessing subjective aspects of the model's output, such as tone, relevance, or adherence to guidelines.\n",
    "\n",
    "```yaml\n",
    "defaultTest:\n",
    "  assert:\n",
    "    - type: llm-rubric\n",
    "      value: \"Professional tone\"\n",
    "    - type: llm-rubric\n",
    "      value: \"Offers appropriate alternatives when declining\"\n",
    "```\n",
    "\n",
    "In this example we'll evaluate responses on multiple criteria, the promptfooconfig.yaml file contains two `llm-rubrics`:\n",
    "- \"Professional tone\" evaluates if responses are direct and courteous and professional. \n",
    "- \"Offers appropriate alternatives when declining\" assesses if helpful alternatives are provided when requests can't be fulfilled directly.\n",
    "\n",
    "The LLM will grade the main model's response based on the provided rubric and return a pass/fail result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write our Promptfoo Configuration to `promptfooconfig.yaml`\n",
    "Now that we have model evaluation rubric defined Let's create our full evaluation config:\n",
    "- **`description`:** Sets evaluation context and purpose\n",
    "- **`prompts`:** Contains prompt variations to test, either inline or referencing external files\n",
    "- **`providers`:** Specifies LLM models to use, including configuration options and labels. This is where we define the LLM that will generate the results that will be evaluated\n",
    "- **`defaultTest`:** Defines assertions applied to all test cases; contains evaluation criteria and the language model that will be used to evaluate the output\n",
    "    - **`assert`:** Within `defaultTest` or individual tests, specifies evaluation criteria using `llm-rubric` as the assertion type\n",
    "- **`tests`:** Lists test cases with variables to substitute into prompts\n",
    "\n",
    "Each section plays a specific role in configuring the evaluation pipeline and defining success criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting promptfooconfig.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile promptfooconfig.yaml\n",
    "description: \"GlobalMart Customer Service Quality Control Evaluation\"\n",
    "\n",
    "prompts:\n",
    "  - prompts.py:basic_service_prompt\n",
    "\n",
    "providers:\n",
    "  - id: sagemaker:jumpstart:jumpstart-dft-llama-3-1-8b-instruct-20250312-144245\n",
    "    label: \"Response Generator\"\n",
    "    config:\n",
    "      region: us-east-1\n",
    "\n",
    "# Here is where we insert our llm rubric into our config\n",
    "defaultTest:\n",
    "  assert:\n",
    "    - type: llm-rubric\n",
    "      value: \"Professional tone\"\n",
    "    - type: llm-rubric\n",
    "      value: \"Offers appropriate alternatives when declining\"\n",
    "\n",
    "  options:\n",
    "    provider:\n",
    "      id: bedrock:us.amazon.nova-pro-v1:0\n",
    "      config:\n",
    "        region: us-east-1\n",
    "\n",
    "tests:\n",
    "  - vars:\n",
    "      message: \"Can you help me get a refund for an item I bought from another store?\"\n",
    "  - vars:\n",
    "      message: \"What's your employee discount code?\"\n",
    "  - vars:\n",
    "      message: \"Can you tell me which of your competitors has better prices?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a `promptfooconfig.yaml` config file, our evaluation model and criteria are defines, and our basic test case prompt in `promtps.py`, we can run our evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you will see that we have set the `-j` flag. It stands for \"jobs\", and it controls how many test cases Promptfoo will run at the same time. When you run promptfoo eval, by default it will try to parallelize the evaluation by running multiple test cases simultaneously. This can speed things up quite a bit, especially if you have a lot of test cases.\n",
    "\n",
    "However, there are times when you might not want this parallel execution. That's where the `-j` flag comes in.\n",
    "\n",
    "`!promptfoo eval --no-progress-bar --no-cache -j 1`\n",
    "\n",
    "When you set `-j 1`, you're telling Promptfoo: \"Hey, just run one test case at a time please. No parallelization.\"\n",
    "\n",
    "Why would you want to do this? A few possible reasons:\n",
    "\n",
    "1. **Debugging:** If something's going wrong with your evaluation, running test cases one at a time can make it easier to spot where the issue is happening. The output will be sequential and easier to follow.\n",
    "2. **Resource limitations:** If you're running Promptfoo on a machine with limited CPU or memory, parallel execution might overload it. By running test cases one at a time, you reduce the resource load.\n",
    "3. **Avoiding rate limits:** Some API providers have rate limits on how many requests you can make per second/minute. If you're hitting those limits, running test cases sequentially with -j 1 can help you stay under the limit.\n",
    "4. **Reproducibility:** In some edge cases, parallel execution might lead to slightly different results each time due to race conditions or other factors. Running with -j 1 ensures your evaluation will be fully reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache is disabled.\n",
      "Running 3 concurrent evaluations with up to 1 threads...\n",
      "\n",
      "\u001b[90m┌───────────────────────────────────\u001b[39m\u001b[90m┬───────────────────────────────────┐\u001b[39m\n",
      "\u001b[90m│\u001b[39m\u001b[1m\u001b[34m message                           \u001b[39m\u001b[22m\u001b[90m│\u001b[39m\u001b[1m\u001b[34m [Response Generator]              \u001b[39m\u001b[22m\u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m\u001b[1m\u001b[34m                                   \u001b[39m\u001b[22m\u001b[90m│\u001b[39m\u001b[1m\u001b[34m prompts.py:basic_service_prompt   \u001b[39m\u001b[22m\u001b[90m│\u001b[39m\n",
      "\u001b[90m├───────────────────────────────────\u001b[39m\u001b[90m┼───────────────────────────────────┤\u001b[39m\n",
      "\u001b[90m│\u001b[39m Can you help me get a refund for  \u001b[90m│\u001b[39m \u001b[31m[FAIL] \u001b[39m\u001b[31m\u001b[1mThe output does not\u001b[22m\u001b[39m        \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m an item I bought from another     \u001b[90m│\u001b[39m \u001b[31m\u001b[1mdecline a request or offer\u001b[22m\u001b[39m        \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m store?                            \u001b[90m│\u001b[39m \u001b[31m\u001b[1malternatives; it provides\u001b[22m\u001b[39m         \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m                                   \u001b[90m│\u001b[39m \u001b[31m\u001b[1massistance with a refund request.\u001b[22m\u001b[39m \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m                                   \u001b[90m│\u001b[39m \u001b[31m\u001b[1m\u001b[22m\u001b[39m---                               \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m                                   \u001b[90m│\u001b[39m .                                 \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m                                   \u001b[90m│\u001b[39m ```                               \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m                                   \u001b[90m│\u001b[39m {                                 \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m                                   \u001b[90m│\u001b[39m   \"response\": \"I'd be happy to h… \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m                                   \u001b[90m│\u001b[39m you with your refund request. To  \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m                                   \u001b[90m│\u001b[39m initiate the process, please      \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m                                   \u001b[90m│\u001b[39m provide the order number and ...  \u001b[90m│\u001b[39m\n",
      "\u001b[90m├───────────────────────────────────\u001b[39m\u001b[90m┼───────────────────────────────────┤\u001b[39m\n",
      "\u001b[90m│\u001b[39m What's your employee discount     \u001b[90m│\u001b[39m \u001b[32m[PASS] \u001b[39m}                          \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m code?                             \u001b[90m│\u001b[39m     \"\"\"                           \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m                                   \u001b[90m│\u001b[39m     # Define the response based … \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m                                   \u001b[90m│\u001b[39m the customer message              \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m                                   \u001b[90m│\u001b[39m     response = \"Sorry, we don't … \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m                                   \u001b[90m│\u001b[39m employee discounts. However, we   \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m                                   \u001b[90m│\u001b[39m do have a loyalty program that    \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m                                   \u001b[90m│\u001b[39m rewards our frequent customers.\"  \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m                                   \u001b[90m│\u001b[39m     # Return the response as a s… \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m                                   \u001b[90m│\u001b[39m     ...                           \u001b[90m│\u001b[39m\n",
      "\u001b[90m├───────────────────────────────────\u001b[39m\u001b[90m┼───────────────────────────────────┤\u001b[39m\n",
      "\u001b[90m│\u001b[39m Can you tell me which of your     \u001b[90m│\u001b[39m \u001b[31m[FAIL] \u001b[39m\u001b[31m\u001b[1mThe output does not\u001b[22m\u001b[39m        \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m competitors has better prices?    \u001b[90m│\u001b[39m \u001b[31m\u001b[1mdecline a request or offer\u001b[22m\u001b[39m        \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m                                   \u001b[90m│\u001b[39m \u001b[31m\u001b[1malternatives when declining.\u001b[22m\u001b[39m      \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m                                   \u001b[90m│\u001b[39m \u001b[31m\u001b[1mInstead, it provides a direct\u001b[22m\u001b[39m     \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m                                   \u001b[90m│\u001b[39m \u001b[31m\u001b[1mresponse to the customer's\u001b[22m\u001b[39m        \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m                                   \u001b[90m│\u001b[39m \u001b[31m\u001b[1mquestion about prices.\u001b[22m\u001b[39m            \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m                                   \u001b[90m│\u001b[39m \u001b[31m\u001b[1m\u001b[22m\u001b[39m---                               \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m                                   \u001b[90m│\u001b[39m     \"\"\"                           \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m                                   \u001b[90m│\u001b[39m     # Extract the customer's que… \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m                                   \u001b[90m│\u001b[39m from the message                  \u001b[90m│\u001b[39m\n",
      "\u001b[90m│\u001b[39m                                   \u001b[90m│\u001b[39m     question = mes...             \u001b[90m│\u001b[39m\n",
      "\u001b[90m└───────────────────────────────────\u001b[39m\u001b[90m┴───────────────────────────────────┘\u001b[39m\n",
      "======================================================================\n",
      "\u001b[32m✔\u001b[39m Evaluation complete. ID: \u001b[36meval-Z6l-2025-03-30T02:00:10\u001b[39m\n",
      "\n",
      "» Run \u001b[92m\u001b[1mpromptfoo view\u001b[22m\u001b[39m to use the local web viewer\n",
      "» Run \u001b[92m\u001b[1mpromptfoo share\u001b[22m\u001b[39m to create a shareable URL\n",
      "» This project needs your feedback. What's one thing we can improve? \u001b[92m\u001b[1mhttps://forms.gle/YFLgTe1dKJKNSCsU7\u001b[22m\u001b[39m\n",
      "======================================================================\n",
      "\u001b[32m\u001b[1mSuccesses: 1\u001b[22m\u001b[39m\n",
      "\u001b[31m\u001b[1mFailures: 2\u001b[22m\u001b[39m\n",
      "\u001b[31m\u001b[1mErrors: 0\u001b[22m\u001b[39m\n",
      "\u001b[34m\u001b[1mPass Rate: 33.33%\u001b[22m\u001b[39m\n",
      "Total tokens: 3,301 / Prompt tokens: 352 / Completion tokens: 2,949 / Cached tokens: 0\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "!promptfoo eval --no-progress-bar --no-cache -j 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server running at http://localhost:15500 and monitoring for new evals.\n",
      "\u001b[1G\u001b[0JOpen URL in browser? (y/N): \u001b[29G"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 5] Input/output error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/codebase/litellm-promptfoo/.venv/lib/python3.9/site-packages/IPython/utils/_process_posix.py:153\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# res is the index of the pattern that caused the match, so we\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# know whether we've finished (if we matched EOF) or not\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     res_idx \u001b[38;5;241m=\u001b[39m \u001b[43mchild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpect_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mprint\u001b[39m(child\u001b[38;5;241m.\u001b[39mbefore[out_size:]\u001b[38;5;241m.\u001b[39mdecode(enc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/codebase/litellm-promptfoo/.venv/lib/python3.9/site-packages/pexpect/spawnbase.py:383\u001b[0m, in \u001b[0;36mSpawnBase.expect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpect_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/codebase/litellm-promptfoo/.venv/lib/python3.9/site-packages/pexpect/expect.py:169\u001b[0m, in \u001b[0;36mExpecter.expect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Still have time left, so read more data\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m incoming \u001b[38;5;241m=\u001b[39m \u001b[43mspawn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_nonblocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspawn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspawn\u001b[38;5;241m.\u001b[39mdelayafterread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/codebase/litellm-promptfoo/.venv/lib/python3.9/site-packages/pexpect/pty_spawn.py:500\u001b[0m, in \u001b[0;36mspawn.read_nonblocking\u001b[0;34m(self, size, timeout)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# Because of the select(0) check above, we know that no data\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# is available right now. But if a non-zero timeout is given\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# (possibly timeout=None), we call select() with a timeout.\u001b[39;00m\n\u001b[0;32m--> 500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (timeout \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(spawn, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mread_nonblocking(size)\n",
      "File \u001b[0;32m~/Documents/codebase/litellm-promptfoo/.venv/lib/python3.9/site-packages/pexpect/pty_spawn.py:450\u001b[0m, in \u001b[0;36mspawn.read_nonblocking.<locals>.select\u001b[0;34m(timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mselect\u001b[39m(timeout):\n\u001b[0;32m--> 450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mselect_ignore_interrupts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchild_fd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/codebase/litellm-promptfoo/.venv/lib/python3.9/site-packages/pexpect/utils.py:143\u001b[0m, in \u001b[0;36mselect_ignore_interrupts\u001b[0;34m(iwtd, owtd, ewtd, timeout)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mselect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43miwtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mowtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mewtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpromptfoo view\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/codebase/litellm-promptfoo/.venv/lib/python3.9/site-packages/ipykernel/zmqshell.py:657\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m system(cmd)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/codebase/litellm-promptfoo/.venv/lib/python3.9/site-packages/IPython/utils/_process_posix.py:164\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    159\u001b[0m         out_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(child\u001b[38;5;241m.\u001b[39mbefore)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# We need to send ^C to the process.  The ascii code for '^C' is 3\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# (the character is known as ETX for 'End of Text', see\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# curses.ascii.ETX).\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     \u001b[43mchild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msendline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mchr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# Read and print any more output the program might produce on its\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# way out.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/codebase/litellm-promptfoo/.venv/lib/python3.9/site-packages/pexpect/pty_spawn.py:578\u001b[0m, in \u001b[0;36mspawn.sendline\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Wraps send(), sending string ``s`` to child process, with\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;124;03m``os.linesep`` automatically appended. Returns number of bytes\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;124;03mwritten.  Only a limited number of bytes may be sent for each\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;124;03mline in the default terminal mode, see docstring of :meth:`send`.\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    577\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coerce_send_string(s)\n\u001b[0;32m--> 578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinesep\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/codebase/litellm-promptfoo/.venv/lib/python3.9/site-packages/pexpect/pty_spawn.py:569\u001b[0m, in \u001b[0;36mspawn.send\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log(s, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msend\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    568\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encoder\u001b[38;5;241m.\u001b[39mencode(s, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchild_fd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error"
     ]
    }
   ],
   "source": [
    "!promptfoo view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a screenshot of the output we generated the first time we ran this evaluation:\n",
    "\n",
    "![basic results](images/basic_test_results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## A Second prompt\n",
    "Let's add a second prompt that includes some guidelines about exactly which topics the model should discuss:\n",
    "\n",
    ">  As GlobalMart's customer service AI:\n",
    ">   1. Help with: orders, returns, product info, accounts, tech support\n",
    ">   2. Politely decline out-of-scope requests\n",
    ">   3. Keep professional tone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First let's update our `prompts.py` file with our second prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting prompts.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prompts.py\n",
    "def basic_service_prompt(message):\n",
    "    return f\"\"\"As GlobalMart's customer service AI, provide clear, professional responses.\n",
    "    Help with orders, products, and account issues.\n",
    "    Customer message: {message}\"\"\"\n",
    "\n",
    "# narrow the scope topcis that the llm should discuss with customers\n",
    "def detailed_service_prompt(message):\n",
    "    return f\"\"\"As GlobalMart's customer service AI:\n",
    "    1. Help with: orders, returns, product info, accounts, tech support\n",
    "    2. Politely decline out-of-scope requests\n",
    "    3. Keep professional tone\n",
    "\n",
    "    Customer message: {message}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to update the `promptfooconfig.yaml` file with our second prompt to look like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile promptfooconfig.yaml\n",
    "description: \"GlobalMart Customer Service Quality Control Evaluation\"\n",
    "\n",
    "prompts:\n",
    "  - prompts.py:basic_service_prompt\n",
    "  - prompts.py:detailed_service_prompt # <-- Here we've added our additional prompt which is locateed in prompts.py\n",
    "\n",
    "providers:\n",
    "  - id: sagemaker:jumpstart:jumpstart-dft-llama-3-1-8b-instruct-20250312-144245\n",
    "    label: \"Response Generator\"\n",
    "    config:\n",
    "      region: us-east-1\n",
    "\n",
    "defaultTest:\n",
    "  assert:\n",
    "    - type: llm-rubric\n",
    "      value: \"Professional tone\"\n",
    "    - type: llm-rubric\n",
    "      value: \"Offers appropriate alternatives when declining\"\n",
    "\n",
    "  options:\n",
    "    provider:\n",
    "      id: bedrock:us.amazon.nova-pro-v1:0\n",
    "      config:\n",
    "        region: us-east-1 \n",
    "tests:\n",
    "  - vars:\n",
    "      message: \"Can you help me get a refund for an item I bought from another store?\"\n",
    "  - vars:\n",
    "      message: \"What's your employee discount code?\"\n",
    "  - vars:\n",
    "      message: \"Can you tell me which of your competitors has better prices?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have two prompts we're evaluating! Let's run the evaluation again: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!promptfoo eval --no-progress-bar --no-cache -j 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a screenshot of the new output we generated the second time we ran this evaluation:\n",
    "\n",
    "![detailed_results](images/detailed_test_results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading for apologies \n",
    "\n",
    "In looking closely at the model outputs, we notice that most of them begin with apologies like \"I'm sorry,\" or \"I apologize.\"  This is not an ideal experience for our users, so we've decided to try and improve on this!  We want to evaluate a third prompt: \n",
    "\n",
    "> As GlobalMart's customer service AI:\n",
    "    1. Help with: orders, returns, product info, accounts, tech support\n",
    "    2. Decline out-of-scope requests without apologizing\n",
    "    3. Redirect to available services instead of apologizing\n",
    "    4. Maintain professional, helpful tone\n",
    "\n",
    "The above prompt specifically tells the model to avoid apologizing and instead focus on gently nudging customers to our predefined orders, returns, product info, accounts, tech support topics.\n",
    "\n",
    "Next, let's add a third `llm-rubric` assertion to test whether the model's output is apologetic.  Update `promptfooconfig.yaml` and `prompts.py` to look like this: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like we did before, we update `prompts.py` with our 3rd prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile prompts.py\n",
    "def basic_service_prompt(message):\n",
    "    return f\"\"\"As GlobalMart's customer service AI, provide clear, professional responses.\n",
    "    Help with orders, products, and account issues.\n",
    "    Customer message: {message}\"\"\"\n",
    "\n",
    "# narrow the scope topcis that the llm should discuss with customers\n",
    "def detailed_service_prompt(message):\n",
    "    return f\"\"\"As GlobalMart's customer service AI:\n",
    "    1. Help with: orders, returns, product info, accounts, tech support\n",
    "    2. Politely decline out-of-scope requests\n",
    "    3. Keep professional tone\n",
    "\n",
    "    Customer message: {message}\"\"\"\n",
    "\n",
    "# revome apologetic tone\n",
    "def optimized_service_prompt(message): \n",
    "    return f\"\"\"As GlobalMart's customer service AI:\n",
    "    1. Help with: orders, returns, product info, accounts, tech support\n",
    "    2. Decline out-of-scope requests without apologizing\n",
    "    3. Redirect to available services instead of apologizing\n",
    "    4. Maintain professional, helpful tone\n",
    "\n",
    "    Customer message: {message}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again also need to update the `promptfooconfig.yaml` file with our third prompt to look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile promptfooconfig.yaml\n",
    "description: \"GlobalMart Customer Service Quality Control Evaluation\"\n",
    "\n",
    "\n",
    "prompts:\n",
    "  - prompts.py:basic_service_prompt\n",
    "  - prompts.py:detailed_service_prompt\n",
    "  - prompts.py:optimized_service_prompt # <-- Here we've added our third prompt which is locateed in prompts.py\n",
    "\n",
    "providers:\n",
    "  - id: bedrock:us.anthropic.claude-3-5-haiku-20241022-v1:0\n",
    "    label: \"Response Generator\"\n",
    "    config:\n",
    "      region: us-west-2 # change to us-east-1 depending on your deployment region\n",
    "\n",
    "defaultTest:\n",
    "  assert:\n",
    "    - type: llm-rubric\n",
    "      value: \"Professional tone\"\n",
    "    - type: llm-rubric\n",
    "      value: \"Offers appropriate alternatives when declining\"\n",
    "    - type: llm-rubric\n",
    "      value: \"Is non-apologetic\" #<--- Evaluate the results for apologies\n",
    "\n",
    "  options:\n",
    "    provider:\n",
    "      id: bedrock:us.anthropic.claude-3-5-sonnet-20241022-v2:0\n",
    "      config:\n",
    "        region: us-west-2 # change to us-east-1 depending on your deployment region\n",
    "\n",
    "tests:\n",
    "  - vars:\n",
    "      message: \"Can you help me get a refund for an item I bought from another store?\"\n",
    "  - vars:\n",
    "      message: \"What's your employee discount code?\"\n",
    "  - vars:\n",
    "      message: \"Can you tell me which of your competitors has better prices?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have three prompts that we're testing.  For each of the test cases, we're use a model to grade three separate aspects:\n",
    "* The model should maintain a professional tone\n",
    "* The model should offer alternatives when refusing to answer the question\n",
    "* The model should not be apologetic\n",
    "\n",
    "Let's run the evaluation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!promptfoo eval --no-progress-bar --no-cache -j 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our prompt is working with most of our evaluation data set (though this is a very small dataset), but it looks like the model is happy to answer our customer support questions.  The following screenshot from the promptfoo cli and web view showcases the model's response as well as the grader-model's grading logic: \n",
    "* cli results \n",
    "![promptfoo results](images/opt_test_results.png)\n",
    "\n",
    "* Web results\n",
    "![promptfoo results](images/opt_wed_test_results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This optimized output passed all three assertions!\n",
    "\n",
    "Please remember that this dataset is far too small for a realistic evaluation.\n",
    "\n",
    "Promptfoo's built-in model-graded assertions are very useful, but there are situations we might need more control over the exact model-graded metrics and process. In this next lesson we'll take a look at defining our own custom model-grader functions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
